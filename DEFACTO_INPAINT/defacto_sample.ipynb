{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 改ざん画像データセットのダウンロード・解凍\n",
    "- 前回の試行の続きを行いたい場合（再開モードの場合）は実行不要です．\n",
    "- これは本来のDEFACTOではなく，物体削除系の改ざん画像のみを取り出した簡易版です．\n",
    "- 改ざん領域を含む128x128ピクセルの部分を切り出して使用しています（元々の改ざん領域のサイズが128x128より大きいものは無視しました）．\n",
    "- ファイルサイズが大きいので，必要になるまでダウンロードしないほうが良いです．\n",
    "- 数十分～1時間程度かかる可能性があります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "\n",
    "\n",
    "def make_defacto_tensors(filename, dirname='./defacto'):\n",
    "    transform = transforms.Grayscale()\n",
    "    dataset = CSVBasedDataset(dirname=dirname, filename=os.path.join(dirname, filename), items=['Input Image', 'Ground Truth'], dtypes=['image', 'image'])\n",
    "    data = [dataset[i] for i in range(len(dataset))]\n",
    "    x = torch.cat([torch.unsqueeze(u, dim=0) for u, v in data], dim=0)\n",
    "    y = torch.cat([torch.unsqueeze(transform(v), dim=0) for u, v in data], dim=0)\n",
    "    del dataset, data\n",
    "    return x, y\n",
    "\n",
    "if not os.path.isfile('defacto_train_input_images.pt'):\n",
    "\n",
    "    !wget \"https://tus.box.com/shared/static/v4giouhhlttk29vgoaxuf56rsuzcjkc2.gz\" -O defacto.tar.gz\n",
    "    !tar -zxf defacto.tar.gz\n",
    "    !rm -f defacto.tar.gz\n",
    "    image_tensor, label_tensor = make_defacto_tensors(filename='train_list.csv')\n",
    "    torch.save(image_tensor, './Datasets/defacto_train_input_images.pt')\n",
    "    torch.save(label_tensor, './Datasets/defacto_train_target_images.pt')\n",
    "    del image_tensor, label_tensor\n",
    "    image_tensor, label_tensor = make_defacto_tensors(filename='test_list.csv')\n",
    "    torch.save(image_tensor, './Datasets/defacto_test_input_images.pt')\n",
    "    torch.save(label_tensor, './Datasets/defacto_test_target_images.pt')\n",
    "    torch.save(image_tensor, './temp/defacto_test_input_images.pt')\n",
    "    torch.save(label_tensor, './temp/defacto_test_target_images.pt')\n",
    "    del image_tensor, label_tensor\n",
    "\n",
    "    !rm -fr defacto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### データセットの場所やバッチサイズなどの定数値の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "import torch\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる．\n",
    "# なお，Colab環境で再開モードを利用する場合は，前回終了時に temp ディレクトリの中身を自分の Google Drive に退避しておき，\n",
    "# それを改めて /content/AI_advanced/temp 以下にあらかじめ移しておく必要がある．\n",
    "RESTART_MODE = False\n",
    "\n",
    "\n",
    "# 使用するデバイス\n",
    "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする．\n",
    "# GPU が複数存在する環境では，'cuda:0', 'cuda:1', 'cuda:2' などのような形で使用するGPUのIDを指定する．\n",
    "# Google Colab, Paperspace Gradient などで GPU を利用する場合は DEVICE = 'cuda:0' とすれば良いはず．\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# 高速化・省メモリ化のために半精度小数を用いた混合精度学習を行うか否か（Trueの場合は行う）\n",
    "USE_AMP = True\n",
    "FLOAT_DTYPE = torch.float16 # 混合精度学習を行う場合の半精度小数の型．環境によっては torch.bfloat16 にした方が良好な性能になる（ただしColabのT4 GPU環境ではムリ）．\n",
    "\n",
    "# 混合精度学習の設定\n",
    "if DEVICE == 'cpu':\n",
    "    USE_AMP = False # CPU使用時は強制的に混合精度学習をOFFにする\n",
    "LOSS_SCALER = torch.amp.grad_scaler.GradScaler(enabled=USE_AMP, device='cuda', init_scale=2**16)\n",
    "ADAM_EPS = 1e-4 if USE_AMP and (FLOAT_DTYPE == torch.float16) else 1e-8\n",
    "\n",
    "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
    "# 再開モードの場合も, このエポック数の分だけ追加学習される（N_EPOCHSは最終エポック番号ではない）\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# データセットの存在するフォルダ・ファイル名\n",
    "DATA_DIR = './Datasets/'\n",
    "TRAIN_INPUT_IMAGES_FILE  = 'defacto_train_input_images.pt'\n",
    "TRAIN_TARGET_IMAGES_FILE = 'defacto_train_target_images.pt'\n",
    "VALID_INPUT_IMAGES_FILE  = 'defacto_valid_input_images.pt'\n",
    "VALID_TARGET_IMAGES_FILE = 'defacto_valid_target_images.pt'\n",
    "TEST_INPUT_IMAGES_FILE   = 'defacto_test_input_images.pt'\n",
    "TEST_TARGET_IMAGES_FILE  = 'defacto_test_target_images.pt'\n",
    "\n",
    "# 画像サイズ\n",
    "H = 128 # 縦幅\n",
    "W = 128 # 横幅\n",
    "C = 3 # 入力画像のチャンネル数（カラー画像なら3，グレースケール画像なら1．なお，正解のマスク画像のチャンネル数は常に1）\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './defacto_models/'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, 'forgery_detector_model.pth')\n",
    "\n",
    "# 中断／再開の際に用いる一時ファイル\n",
    "CHECKPOINT_EPOCH = os.path.join('./temp/', 'checkpoint_epoch.pkl')\n",
    "CHECKPOINT_MODEL = os.path.join('./temp/', 'checkpoint_model.pth')\n",
    "CHECKPOINT_OPT = os.path.join('./temp/', 'checkpoint_opt.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ニューラルネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 畳込み，バッチ正規化，ReLUをセットで行うクラス\n",
    "class myConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(myConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "# 逆畳込み，バッチ正規化，ReLUをセットで行うクラス\n",
    "class myConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(myConvTranspose2d, self).__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "# defacto の画像中から改ざん領域を推定するニューラルネットワーク\n",
    "class ForgeryDetector(nn.Module):\n",
    "\n",
    "    # C: 入力画像のチャンネル数（1または3と仮定）\n",
    "    # H: 入力画像の縦幅（8の倍数と仮定）\n",
    "    # W: 入力画像の横幅（8の倍数と仮定）\n",
    "    def __init__(self, C, H, W):\n",
    "        super(ForgeryDetector, self).__init__()\n",
    "\n",
    "        # 畳込み層1\n",
    "        # カーネルサイズ3，ストライド幅1，パディング1の設定なので，これを通しても特徴マップの縦幅・横幅は変化しない\n",
    "        self.conv1 = myConv2d(in_channels=C, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # 畳込み層2～4\n",
    "        # カーネルサイズ4，ストライド幅2，パディング1の設定なので，これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 1/2 になる\n",
    "        # 3つ適用することになるので，最終的には都合 1/8 になる -> ゆえに，入力画像の縦幅と横幅を各々8の倍数と仮定している\n",
    "        self.conv2 = myConv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = myConv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = myConv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # 逆畳込み層5～7\n",
    "        # カーネルサイズ4，ストライド幅2，パディング1の設定なので，これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 2 倍になる\n",
    "        # 3つ適用することになるので，最終的には元の大きさに戻る\n",
    "        self.deconv5 = myConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv6 = myConvTranspose2d(in_channels=128, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv7 = myConvTranspose2d(in_channels=64, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # 畳込み層8\n",
    "        # カーネルサイズ3，ストライド幅1，パディング1の設定なので，これを通しても特徴マップの縦幅・横幅は変化しない\n",
    "        self.conv8 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.conv1(x)\n",
    "        h2 = self.conv2(h1)\n",
    "        h3 = self.conv3(h2)\n",
    "        h4 = self.conv4(h3)\n",
    "        h = self.deconv5(h4)\n",
    "        h = torch.cat([h, h3], dim=1) # U-net 型の skip connection\n",
    "        h = self.deconv6(h)\n",
    "        h = torch.cat([h, h2], dim=1) # U-net 型の skip connection\n",
    "        h = self.deconv7(h)\n",
    "        h = torch.cat([h, h1], dim=1) # U-net 型の skip connection\n",
    "        y = torch.sigmoid(self.conv8(h))\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from mylib.data_io import TensorDataset\n",
    "\n",
    "\n",
    "# 再開モードの場合は，前回使用したデータセットをロードして使用する\n",
    "if RESTART_MODE:\n",
    "\n",
    "    # テンソルファイルを読み込み，前回使用したデータセットを用意\n",
    "    train_dataset = TensorDataset(filenames=[\n",
    "        os.path.join('./temp/', TRAIN_INPUT_IMAGES_FILE),\n",
    "        os.path.join('./temp/', TRAIN_TARGET_IMAGES_FILE)\n",
    "    ])\n",
    "    valid_dataset = TensorDataset(filenames=[\n",
    "        os.path.join('./temp/', VALID_INPUT_IMAGES_FILE),\n",
    "        os.path.join('./temp/', VALID_TARGET_IMAGES_FILE)\n",
    "    ])\n",
    "    train_size = len(train_dataset)\n",
    "    valid_size = len(valid_dataset)\n",
    "\n",
    "# そうでない場合は，データセットを読み込む\n",
    "else:\n",
    "\n",
    "    # テンソルファイルを読み込み, 訓練データセットを用意\n",
    "    dataset = TensorDataset(filenames=[\n",
    "        os.path.join(DATA_DIR, TRAIN_INPUT_IMAGES_FILE),\n",
    "        os.path.join(DATA_DIR, TRAIN_TARGET_IMAGES_FILE)\n",
    "    ])\n",
    "\n",
    "    # 訓練データセットを分割し，一方を検証用に回す\n",
    "    dataset_size = len(dataset)\n",
    "    valid_size = int(0.01 * dataset_size) # 全体の 1% を検証用に\n",
    "    train_size = dataset_size - valid_size # 残りの 99% を学習用に\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "    # データセット情報をファイルに保存\n",
    "    torch.save(torch.cat([torch.unsqueeze(train_dataset[i][0], dim=0) for i in range(len(train_dataset))], dim=0), os.path.join('./temp/', TRAIN_INPUT_IMAGES_FILE))\n",
    "    torch.save(torch.cat([torch.unsqueeze(train_dataset[i][1], dim=0) for i in range(len(train_dataset))], dim=0), os.path.join('./temp/', TRAIN_TARGET_IMAGES_FILE))\n",
    "    torch.save(torch.cat([torch.unsqueeze(valid_dataset[i][0], dim=0) for i in range(len(valid_dataset))], dim=0), os.path.join('./temp/', VALID_INPUT_IMAGES_FILE))\n",
    "    torch.save(torch.cat([torch.unsqueeze(valid_dataset[i][1], dim=0) for i in range(len(valid_dataset))], dim=0), os.path.join('./temp/', VALID_TARGET_IMAGES_FILE))\n",
    "\n",
    "\n",
    "# 訓練データおよび検証用データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 性能評価のための関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 二枚の白黒画像の一致度を調べる\n",
    "def region_consistency_metric(estimated, gt, threshold=0.2):\n",
    "\n",
    "    def ratio(a, b):\n",
    "        indices = b.nonzero()\n",
    "        if len(indices) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(torch.mean(a[indices] / b[indices]))\n",
    "\n",
    "    one = torch.ones(gt.size()).to(gt.device)\n",
    "    zero = torch.zeros(gt.size()).to(gt.device)\n",
    "    gt = torch.where(gt > threshold, one, zero)\n",
    "    estimated = torch.where(estimated > threshold, one, zero)\n",
    "    intersection = estimated * gt\n",
    "    union = estimated + gt\n",
    "    union = torch.where(union > 1, one, union)\n",
    "    E = torch.sum(estimated, dim=(1, 2, 3))\n",
    "    G = torch.sum(gt, dim=(1, 2, 3))\n",
    "    I = torch.sum(intersection, dim=(1, 2, 3))\n",
    "    U = torch.sum(union, dim=(1, 2, 3))\n",
    "    recall = ratio(I, G)\n",
    "    precision = ratio(I, E)\n",
    "    iou = ratio(I, U)\n",
    "\n",
    "    return recall, precision, iou"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from mylib.visualizers import LossVisualizer\n",
    "from mylib.data_io import show_images\n",
    "from mylib.utility import save_checkpoint, load_checkpoint\n",
    "\n",
    "\n",
    "# エポック番号\n",
    "INIT_EPOCH = 0 # 初期値\n",
    "LAST_EPOCH = INIT_EPOCH + N_EPOCHS # 最終値\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "model = ForgeryDetector(C=C, H=H, W=W).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは SGD でなく Adam を使用）\n",
    "optimizer = optim.Adam(model.parameters(), eps=ADAM_EPS)\n",
    "\n",
    "# 再開モードの場合は，前回チェックポイントから情報をロードして学習再開\n",
    "if RESTART_MODE:\n",
    "    INIT_EPOCH, LAST_EPOCH, model, optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_MODEL, CHECKPOINT_OPT, N_EPOCHS, model, optimizer)\n",
    "    print('')\n",
    "\n",
    "# 損失関数\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "# 損失関数値を記録する準備\n",
    "loss_viz = LossVisualizer(['train loss', 'valid loss', 'recall', 'precision', 'IoU'], init_epoch=INIT_EPOCH)\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "for epoch in range(INIT_EPOCH, LAST_EPOCH):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    for X, Y in tqdm(train_dataloader):\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        X = X.to(DEVICE) # 入力画像\n",
    "        Y = Y.to(DEVICE) # 正解のマスク画像\n",
    "        with torch.amp.autocast_mode.autocast(enabled=USE_AMP, device_type='cuda', dtype=FLOAT_DTYPE):\n",
    "            Y_pred = model(X) # 入力画像 X をニューラルネットワークに入力し，改ざん領域の推測値 Y_pred を得る\n",
    "        loss = loss_func(Y_pred.to(torch.float32), Y) # 損失関数の現在値を計算\n",
    "        with torch.amp.autocast_mode.autocast(enabled=USE_AMP, device_type='cuda', dtype=FLOAT_DTYPE):\n",
    "            LOSS_SCALER.scale(loss).backward() # 誤差逆伝播法により，個々のパラメータに関する損失関数の勾配（偏微分）を計算\n",
    "            LOSS_SCALER.step(optimizer)\n",
    "            LOSS_SCALER.update() # 勾配に沿ってパラメータの値を更新\n",
    "            sum_loss += float(loss) * len(X)\n",
    "    avg_loss = sum_loss / train_size\n",
    "    loss_viz.add_value('train loss', avg_loss) # 訓練データに対する損失関数の値を記録\n",
    "    print('train loss = {0:.6f}'.format(avg_loss))\n",
    "\n",
    "    # 検証\n",
    "    model.eval()\n",
    "    sum_loss = 0\n",
    "    sum_recall = 0\n",
    "    sum_precision = 0\n",
    "    sum_IoU = 0\n",
    "    with torch.inference_mode():\n",
    "        for X, Y in tqdm(valid_dataloader):\n",
    "            X = X.to(DEVICE) # 入力画像\n",
    "            Y = Y.to(DEVICE) # 正解のマスク画像\n",
    "            Y_pred = model(X)\n",
    "            loss = loss_func(Y_pred, Y)\n",
    "            recall, precision, IoU = region_consistency_metric(Y_pred, Y) # 評価指標の値を計算\n",
    "            sum_recall += recall * len(X)\n",
    "            sum_precision += precision * len(X)\n",
    "            sum_IoU += IoU * len(X)\n",
    "            sum_loss += float(loss) * len(X)\n",
    "    avg_recall = sum_recall / valid_size\n",
    "    avg_precision = sum_precision / valid_size\n",
    "    avg_IoU = sum_IoU / valid_size\n",
    "    avg_loss = sum_loss / valid_size\n",
    "    loss_viz.add_value('valid loss', avg_loss) # 検証用データに対する損失関数の値を記録\n",
    "    loss_viz.add_value('recall', avg_recall) # 検証用データに対する評価指標の値を記録\n",
    "    loss_viz.add_value('precision', avg_precision) # 同上\n",
    "    loss_viz.add_value('IoU', avg_IoU) # 同上\n",
    "    print('valid loss = {0:.6f}'.format(avg_loss))\n",
    "    print('recall = {0:.6f}'.format(avg_recall))\n",
    "    print('precision = {0:.6f}'.format(avg_precision))\n",
    "    print('IoU = {0:.6f}'.format(avg_IoU))\n",
    "    print('')\n",
    "\n",
    "    # 学習経過の表示\n",
    "    if epoch == 0:\n",
    "        show_images(Y.to('cpu').detach(), num=8, num_per_row=8, title='ground truth', save_fig=False, save_dir=MODEL_DIR)\n",
    "    show_images(Y_pred.to('cpu').detach(), num=8, num_per_row=8, title='epoch {0}'.format(epoch + 1), save_fig=False, save_dir=MODEL_DIR)\n",
    "\n",
    "    # 現在の学習状態を一時ファイルに保存\n",
    "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_MODEL, CHECKPOINT_OPT, epoch+1, model, optimizer)\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "model = model.to('cpu')\n",
    "torch.save(model.state_dict(), MODEL_FILE)\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'loss_history.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 必要なら，現在の学習経過情報を自分のgoogle driveに退避\n",
    "- Google Colab以外の環境で使用することは想定していません．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# 退避先フォルダの名称\n",
    "# 自分のgoogle driveにおけるルートフォルダの直下にこの名前のフォルダが作成される\n",
    "# 既に存在する場合は一度削除した上で再作成される（削除したフォルダはgoogle driveのゴミ箱に移動するようです．必要に応じて完全に削除して下さい）\n",
    "DST_DIR_NAME = 'AI_advanced_temp'\n",
    "\n",
    "# tempフォルダをまるごとgoogle driveに退避\n",
    "MOUNT_POINT = '/content/drive'\n",
    "DST_DIR_PATH = os.path.join(MOUNT_POINT, 'MyDrive', DST_DIR_NAME)\n",
    "if not os.path.isdir(MOUNT_POINT):\n",
    "    drive.mount(MOUNT_POINT)\n",
    "if os.path.exists(DST_DIR_PATH):\n",
    "    !rm -r $DST_DIR_PATH\n",
    "!cp -r temp $DST_DIR_PATH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習済みニューラルネットワークモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
    "model = ForgeryDetector(C=C, H=H, W=W)\n",
    "model.load_state_dict(torch.load(MODEL_FILE, weights_only=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### テストデータセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from mylib.data_io import TensorDataset\n",
    "\n",
    "\n",
    "# テンソルファイルを読み込み, 訓練データセットを用意\n",
    "if RESTART_MODE:\n",
    "    test_dataset = TensorDataset(filenames=[\n",
    "        os.path.join('./temp/', TEST_INPUT_IMAGES_FILE),\n",
    "        os.path.join('./temp/', TEST_TARGET_IMAGES_FILE)\n",
    "    ])\n",
    "else:\n",
    "    test_dataset = TensorDataset(filenames=[\n",
    "        os.path.join(DATA_DIR, TEST_INPUT_IMAGES_FILE),\n",
    "        os.path.join(DATA_DIR, TEST_TARGET_IMAGES_FILE)\n",
    "    ])\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "# テストデータをミニバッチに分けて使用するための「データローダ」を用意\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### テスト処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mylib.data_io import show_images\n",
    "\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# テストデータセットで精度を評価\n",
    "sum_recall = 0\n",
    "sum_precision = 0\n",
    "sum_IoU = 0\n",
    "with torch.inference_mode():\n",
    "    for X, Y in tqdm(test_dataloader):\n",
    "        X = X.to(DEVICE) # 入力画像\n",
    "        Y = Y.to(DEVICE) # 正解のマスク画像\n",
    "        with torch.amp.autocast_mode.autocast(enabled=USE_AMP, device_type='cuda', dtype=FLOAT_DTYPE):\n",
    "            Y_pred = model(X)\n",
    "        recall, precision, IoU = region_consistency_metric(Y_pred.to(torch.float32), Y)\n",
    "        sum_recall += recall * len(X)\n",
    "        sum_precision += precision * len(X)\n",
    "        sum_IoU += IoU * len(X)\n",
    "avg_recall = sum_recall / test_size\n",
    "avg_precision = sum_precision / test_size\n",
    "avg_IoU = sum_IoU / test_size\n",
    "print('recall = {0:.6f}'.format(avg_recall))\n",
    "print('precision = {0:.6f}'.format(avg_precision))\n",
    "print('IoU = {0:.6f}'.format(avg_IoU))\n",
    "print('')\n",
    "\n",
    "# 推定結果の例を表示\n",
    "show_images(Y.to('cpu').detach(), num=32, num_per_row=8, title='ground truth', save_fig=False, save_dir=MODEL_DIR)\n",
    "show_images(Y_pred.to('cpu').detach(), num=32, num_per_row=8, title='estimated', save_fig=False, save_dir=MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
