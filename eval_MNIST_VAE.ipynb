{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307b4169",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.manifold import TSNE\n",
    "from src.VAE_MNIST import MNISTEncoderV, MNISTDecoder\n",
    "from src.utils.data_io import show_images, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8bd23e",
   "metadata": {},
   "source": [
    "### Conig parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32835ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "N = 32\n",
    "MODEL_DIR_VAE = \"./models/MNIST/VAE/\"\n",
    "MODEL_NAME = \"VAE\"\n",
    "\n",
    "USE_BATCH_NORM = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = './data/MNIST/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b40fa0",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065523ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(n=N, use_batch_norm=USE_BATCH_NORM, device=DEVICE):\n",
    "    \"\"\"Load trained encoder and decoder models\"\"\"\n",
    "    model_file_enc = MODEL_DIR_VAE + f\"{n}_encoder.pth\"\n",
    "    model_file_dec = MODEL_DIR_VAE + f\"{n}_decoder.pth\"\n",
    "    \n",
    "    enc_model = MNISTEncoderV(N=n, use_BatchNorm=use_batch_norm).to(device)\n",
    "    enc_model.load_state_dict(torch.load(model_file_enc, weights_only=True))\n",
    "    enc_model.eval()\n",
    "    \n",
    "    dec_model = MNISTDecoder(N=n, use_BatchNorm=use_batch_norm).to(device)\n",
    "    dec_model.load_state_dict(torch.load(model_file_dec, weights_only=True))\n",
    "    dec_model.eval()\n",
    "    \n",
    "    return enc_model, dec_model\n",
    "\n",
    "enc_model, dec_model = load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1df8e9",
   "metadata": {},
   "source": [
    "### Generate images from random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_from_normal_dist(decoder, n_samples=32, latent_dim=N, z_mu=0, z_sigma=1, device=DEVICE):\n",
    "    \"\"\"Generate images by sampling from latent space\"\"\"\n",
    "    z = z_sigma * torch.randn((n_samples, latent_dim)).to(device) + z_mu\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        generated_images = decoder(z)\n",
    "        show_images(generated_images.to('cpu').detach(), num=n_samples, \n",
    "                   num_per_row=8, title=f'{N}_{MODEL_NAME}_generated_samples', save_fig=True, save_dir=f\"./results/MNIST/{MODEL_NAME}/\")\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "gen_samples = generate_samples_from_normal_dist(dec_model, n_samples=32, latent_dim=N, z_mu=0, z_sigma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c869d3d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(data_dir=DATA_DIR):\n",
    "    \"\"\"Load MNIST dataset from files\"\"\"\n",
    "    train_images = torch.load(os.path.join(data_dir, 'MNIST_train_images.pt'))\n",
    "    train_labels = torch.load(os.path.join(data_dir, 'MNIST_train_labels.pt'))\n",
    "    return train_images, train_labels\n",
    "\n",
    "train_images, train_labels = load_mnist_data()\n",
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afa5d2",
   "metadata": {},
   "source": [
    "### Select a balance of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee797cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_balanced_samples(images, labels, samples_per_class=100):\n",
    "    \"\"\"Select equal number of samples from each class\"\"\"\n",
    "    selected_indices = torch.cat([torch.where(labels == label)[0][:samples_per_class] \n",
    "                                 for label in range(10)])\n",
    "    \n",
    "    selected_images = images[selected_indices]\n",
    "    selected_labels = labels[selected_indices]\n",
    "    \n",
    "    print(f\"Selected images shape: {selected_images.shape}\")\n",
    "    \n",
    "    # Verify class distribution\n",
    "    unique_labels, counts = torch.unique(selected_labels, return_counts=True)\n",
    "    for label, count in zip(unique_labels.tolist(), counts.tolist()):\n",
    "        print(f\"Label {label} count: {count}\")\n",
    "    \n",
    "    return selected_images, selected_labels\n",
    "\n",
    "selected_images, selected_labels = select_balanced_samples(train_images, train_labels, samples_per_class=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37851ed2",
   "metadata": {},
   "source": [
    "### Visualize the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space(encoder, images, labels, device=DEVICE):\n",
    "    \"\"\"Visualize latent space using t-SNE\"\"\"\n",
    "    # Generate embeddings\n",
    "    with torch.inference_mode():\n",
    "        embedding, mu, lnvar = encoder(images.to(device))\n",
    "    \n",
    "    # Convert to numpy for t-SNE\n",
    "    embedding_np = embedding.cpu().numpy()\n",
    "    \n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    reduced_embedding = tsne.fit_transform(embedding_np)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = [\"red\", \"green\", \"blue\", \"orange\", \"purple\", \n",
    "              \"brown\", \"fuchsia\", \"grey\", \"olive\", \"lightblue\"]\n",
    "    \n",
    "    for point, label in zip(reduced_embedding, labels):\n",
    "        plt.scatter(point[0], point[1], marker=\"${}$\".format(label), \n",
    "                   c=colors[label], alpha=0.7)\n",
    "    \n",
    "    plt.title('t-SNE Visualization of MNIST Latent Space')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.colorbar(label='Label')\n",
    "    \n",
    "    plt.subplots_adjust(left=0.1, right=0.85, top=0.9, bottom=0.1)\n",
    "    save_path = os.path.join(f\"./results/MNIST/{MODEL_NAME}\", f\"{embedding_np.shape[1]}_{MODEL_NAME}_tsne_visualization.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_latent_space(enc_model, selected_images, selected_labels, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483f18f",
   "metadata": {},
   "source": [
    "### morph between two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_between_digits(encoder, decoder, images, labels, \n",
    "                       digit1, digit2, num_steps=10, device=DEVICE):\n",
    "    \"\"\"Generate morphing sequence between two digits\"\"\"\n",
    "    # Find indices for the specified digits\n",
    "    indices1 = torch.where(labels == digit1)[0]\n",
    "    indices2 = torch.where(labels == digit2)[0]\n",
    "    \n",
    "    if len(indices1) == 0 or len(indices2) == 0:\n",
    "        print(f\"Digit {digit1} or {digit2} not found.\")\n",
    "        return\n",
    "    \n",
    "    # Get sample images\n",
    "    img1 = images[indices1[0]].unsqueeze(0).to(device)\n",
    "    img2 = images[indices2[0]].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get latent representations\n",
    "    with torch.inference_mode():\n",
    "        z1, _, _ = encoder(img1)\n",
    "        z2, _, _ = encoder(img2)\n",
    "    \n",
    "    # Prepare visualization\n",
    "    fig, axes = plt.subplots(1, num_steps, figsize=(num_steps * 2, 3))\n",
    "    \n",
    "    # Generate interpolated images\n",
    "    for i in range(num_steps):\n",
    "        alpha = i / (num_steps - 1) if num_steps > 1 else 0\n",
    "        z_interp = (1 - alpha) * z1 + alpha * z2\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            img_interp = decoder(z_interp)\n",
    "        \n",
    "        img_np = img_interp.cpu().squeeze().detach().numpy()\n",
    "        axes[i].imshow(img_np, cmap='gray')\n",
    "        axes[i].set_title(f\"{digit1}â†’{digit2} ({alpha:.1f})\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Morphing from digit {digit1} to {digit2}\")\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(f\"./results/MNIST/{MODEL_NAME}\", f\"{z1.shape[1]}_{MODEL_NAME}_morphing_{digit1}_to_{digit2}.png\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Demonstrate morphing between digits\n",
    "morph_between_digits(enc_model, dec_model, selected_images, selected_labels, digit1=1, digit2=7, num_steps=8)\n",
    "morph_between_digits(enc_model, dec_model, selected_images, selected_labels, digit1=0, digit2=9, num_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00864924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
