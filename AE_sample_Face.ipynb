{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAUf1sKbAg4O"
      },
      "source": [
        "##### 作業用ディレクトリの準備\n",
        "- ローカルのWindows環境で実行する場合は，Git for Windowsを事前にインストールしておく必要があります．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cedNAKCuAg4R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# 独自ライブラリ等のダウンロード\n",
        "if not os.path.isdir('AI_advanced'):\n",
        "    !git clone https://github.com/knakamura1982/AI_advanced.git\n",
        "%cd AI_advanced\n",
        "\n",
        "# モデルファイルの保存先ディレクトリの作成\n",
        "if not os.path.isdir('AE_models_Face'):\n",
        "    !mkdir AE_models_Face\n",
        "\n",
        "# 一時ファイルの保存先ディレクトリの作成\n",
        "if os.path.exists('temp'):\n",
        "    if os.name == 'nt':\n",
        "        !Powershell.exe -Command \"rm -r -fo temp\"\n",
        "    else:\n",
        "        !rm -fr temp\n",
        "!mkdir temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP9UUuKYAg4S"
      },
      "source": [
        "##### 顔画像データセットCelebAのダウンロード・解凍\n",
        "- 前回の試行の続きを行いたい場合（再開モードの場合）は実行不要です．\n",
        "- これは本来のCelebAではなく，その中から10%弱の画像をランダムに抜き出した簡易版です．\n",
        "- 十数分かかる可能性があります．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2IIOaOPAg4S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from mylib.data_io import CSVBasedDataset\n",
        "\n",
        "\n",
        "if not os.path.isfile('./Datasets/tinyCelebA_train_images.pt'):\n",
        "    if os.name == 'nt':\n",
        "        # ローカルのWindows環境の場合\n",
        "        !Powershell.exe -Command \"wget https://tus.box.com/shared/static/z7a4pb9qtco6fwspige2tpt2ryhqv9l1.gz -O tinyCelebA.tar.gz\"\n",
        "        !Powershell.exe -Command \"tar -zxf tinyCelebA.tar.gz\"\n",
        "        !Powershell.exe -Command \"rm -fo tinyCelebA.tar.gz\"\n",
        "    else:\n",
        "        # それ以外（Colab環境含む）の場合\n",
        "        !wget \"https://tus.box.com/shared/static/z7a4pb9qtco6fwspige2tpt2ryhqv9l1.gz\" -O tinyCelebA.tar.gz\n",
        "        !tar -zxf tinyCelebA.tar.gz\n",
        "        !rm -f tinyCelebA.tar.gz\n",
        "    dataset = CSVBasedDataset(\n",
        "        dirname='./tinyCelebA',\n",
        "        filename='./tinyCelebA/image_list.csv',\n",
        "        items=[\n",
        "            'File Path',\n",
        "            [\n",
        "                '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry',\n",
        "                'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
        "                'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair',\n",
        "                'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'\n",
        "            ]\n",
        "        ],\n",
        "        dtypes=['image', 'float'],\n",
        "        img_transform=transforms.CenterCrop((128, 128))\n",
        "    )\n",
        "    data = [dataset[i] for i in range(len(dataset))]\n",
        "    image_tensor = torch.cat([torch.unsqueeze(u, dim=0) for u, v in data], dim=0)\n",
        "    label_tensor = torch.cat([torch.unsqueeze(v, dim=0) for u, v in data], dim=0)\n",
        "    torch.save(image_tensor, './Datasets/tinyCelebA_train_images.pt')\n",
        "    torch.save(label_tensor, './Datasets/tinyCelebA_train_labels.pt')\n",
        "    del dataset, data, image_tensor, label_tensor\n",
        "    if os.name == 'nt':\n",
        "        !Powershell.exe -Command \"rm -r -fo tinyCelebA\"\n",
        "    else:\n",
        "        !rm -fr tinyCelebA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQkmvb4wAg4T"
      },
      "source": [
        "##### データセットの場所やバッチサイズなどの定数値の設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "modaTCUOAg4T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
        "import torch\n",
        "\n",
        "\n",
        "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる．\n",
        "# なお，Colab環境で再開モードを利用する場合は，前回終了時に temp ディレクトリの中身を自分の Google Drive に退避しておき，\n",
        "# それを改めて /content/AI_advanced/temp 以下にあらかじめ移しておく必要がある．\n",
        "RESTART_MODE = False\n",
        "\n",
        "\n",
        "# 使用するデバイス\n",
        "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする．\n",
        "# GPU が複数存在する環境では，'cuda:0', 'cuda:1', 'cuda:2' などのような形で使用するGPUのIDを指定する．\n",
        "# Google Colab, Paperspace Gradient などで GPU を利用する場合は DEVICE = 'cuda:0' とすれば良いはず．\n",
        "DEVICE = 'cuda:0'\n",
        "\n",
        "# 高速化・省メモリ化のために半精度小数を用いた混合精度学習を行うか否か（Trueの場合は行う）\n",
        "USE_AMP = True\n",
        "FLOAT_DTYPE = torch.float16 # 混合精度学習を行う場合の半精度小数の型．環境によっては torch.bfloat16 にした方が良好な性能になる（ただしColabのT4 GPU環境ではムリ）．\n",
        "\n",
        "# 混合精度学習の設定\n",
        "if DEVICE == 'cpu':\n",
        "    USE_AMP = False # CPU使用時は強制的に混合精度学習をOFFにする\n",
        "LOSS_SCALER = torch.amp.grad_scaler.GradScaler(enabled=USE_AMP, device='cuda', init_scale=2**16)\n",
        "ADAM_EPS = 1e-4 if USE_AMP and (FLOAT_DTYPE == torch.float16) else 1e-8\n",
        "\n",
        "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
        "# 再開モードの場合も, このエポック数の分だけ追加学習される（N_EPOCHSは最終エポック番号ではない）\n",
        "N_EPOCHS = 20\n",
        "\n",
        "# 学習時のバッチサイズ\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# データセットの存在するフォルダ・ファイル名\n",
        "DATA_DIR = './Datasets/'\n",
        "TRAIN_IMAGES_FILE = 'tinyCelebA_train_images.pt'\n",
        "VALID_IMAGES_FILE = 'tinyCelebA_valid_images.pt'\n",
        "\n",
        "# 画像サイズ\n",
        "H = 128 # 縦幅\n",
        "W = 128 # 横幅\n",
        "C = 3 # チャンネル数（カラー画像なら3，グレースケール画像なら1）\n",
        "\n",
        "# 特徴ベクトルの次元数\n",
        "N = 128\n",
        "\n",
        "# 学習結果の保存先フォルダ\n",
        "MODEL_DIR = './AE_models_Face/'\n",
        "\n",
        "# 学習結果のニューラルネットワークの保存先\n",
        "MODEL_FILE_ENC = os.path.join(MODEL_DIR, 'face_encoder_model.pth') # エンコーダ\n",
        "MODEL_FILE_DEC = os.path.join(MODEL_DIR, 'face_decoder_model.pth') # デコーダ\n",
        "\n",
        "# 中断／再開の際に用いる一時ファイルの保存先\n",
        "CHECKPOINT_EPOCH = os.path.join('./temp/', 'checkpoint_epoch.pkl')\n",
        "CHECKPOINT_ENC_MODEL = os.path.join('./temp/', 'checkpoint_enc_model.pth')\n",
        "CHECKPOINT_DEC_MODEL = os.path.join('./temp/', 'checkpoint_dec_model.pth')\n",
        "CHECKPOINT_ENC_OPT = os.path.join('./temp/', 'checkpoint_enc_opt.pth')\n",
        "CHECKPOINT_DEC_OPT = os.path.join('./temp/', 'checkpoint_dec_opt.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sylgV1RsAg4T"
      },
      "source": [
        "##### ニューラルネットワークモデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cHtZbd2Ag4U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Residual Block\n",
        "# 入力特徴マップと出力特徴マップのチャンネル数は同一であることを前提とする\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels, kernel_size, stride, padding, activation=F.relu):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.activation = activation\n",
        "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=channels)\n",
        "    def forward(self, x):\n",
        "        h = self.activation(self.bn1(self.conv1(x)))\n",
        "        h = self.bn2(self.conv2(h))\n",
        "        return self.activation(h + x)\n",
        "\n",
        "\n",
        "# 顔画像を N 次元の特徴ベクトルへと圧縮するニューラルネットワーク\n",
        "# AutoEncoderのエンコーダ部分のサンプル\n",
        "class FaceEncoder(nn.Module):\n",
        "\n",
        "    # C: 入力顔画像のチャンネル数（1または3と仮定）\n",
        "    # H: 入力顔画像の縦幅（8の倍数と仮定）\n",
        "    # W: 入力顔画像の横幅（8の倍数と仮定）\n",
        "    # N: 出力の特徴ベクトルの次元数\n",
        "    def __init__(self, C, H, W, N):\n",
        "        super(FaceEncoder, self).__init__()\n",
        "\n",
        "        # 畳込み層1～3\n",
        "        # カーネルサイズ4，ストライド幅2，パディング1の設定なので，これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 1/2 になる\n",
        "        # 3つ適用することになるので，最終的には都合 1/8 になる -> ゆえに，入力顔画像の縦幅と横幅を各々8の倍数と仮定している\n",
        "        self.conv1 = nn.Conv2d(in_channels=C, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        # 畳込み層4\n",
        "        # カーネルサイズ3，ストライド幅1，パディング1の設定なので，これを通しても特徴マップの縦幅・横幅は変化しない\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        #self.conv4 = ResBlock(channels=64, kernel_size=3, stride=1, padding=1) # 例: Residual Block を使用する場合はこのように記載\n",
        "\n",
        "        # バッチ正規化層\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=64)\n",
        "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
        "\n",
        "        # 平坦化\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        # 全結合層1\n",
        "        # 畳込み層1～3を通すことにより特徴マップの縦幅・横幅は都合 1/8 になっている．\n",
        "        # その後，さらに self.conv4 を通してから全結合層を適用する予定なので，入力側のユニット数は 64*(H/8)*(W/8) = H*W\n",
        "        self.fc1 = nn.Linear(in_features=H*W, out_features=2048)\n",
        "\n",
        "        # 全結合層2\n",
        "        self.fc2 = nn.Linear(in_features=2048, out_features=N)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.leaky_relu(self.bn1(self.conv1(x)))\n",
        "        h = F.leaky_relu(self.bn2(self.conv2(h)))\n",
        "        h = F.leaky_relu(self.bn3(self.conv3(h)))\n",
        "        h = F.leaky_relu(self.bn4(self.conv4(h)))\n",
        "        #h = self.conv4(h) # 例: Residual Block を使用する場合はこのように記載（Residual Blockの内部でバッチ正規化と活性化関数を適用しているので，外側では適用しない）\n",
        "        h = self.flat(h)\n",
        "        h = F.leaky_relu(self.fc1(h))\n",
        "        z = self.fc2(h)\n",
        "        return z\n",
        "\n",
        "\n",
        "# N 次元の特徴ベクトルから顔画像を生成するニューラルネットワーク\n",
        "# AutoEncoderのデコーダ部分のサンプル\n",
        "class FaceDecoder(nn.Module):\n",
        "\n",
        "    # C: 出力顔画像のチャンネル数（1または3と仮定）\n",
        "    # H: 出力顔画像の縦幅（8の倍数と仮定）\n",
        "    # W: 出力顔画像の横幅（8の倍数と仮定）\n",
        "    # N: 入力の特徴ベクトルの次元数\n",
        "    def __init__(self, C, H, W, N):\n",
        "        super(FaceDecoder, self).__init__()\n",
        "        self.W = W\n",
        "        self.H = H\n",
        "\n",
        "        # 全結合層1,2\n",
        "        # パーセプトロン数は FaceEncoder の全結合層と真逆に設定\n",
        "        self.fc2 = nn.Linear(in_features=N, out_features=2048)\n",
        "        self.fc1 = nn.Linear(in_features=2048, out_features=H*W)\n",
        "\n",
        "        # 転置畳込み層1～4\n",
        "        # カーネルサイズ，ストライド幅，パディングは FaceEncoder の畳込み層1～4と真逆に設定\n",
        "        self.deconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv1 = nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        # バッチ正規化層\n",
        "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
        "\n",
        "        # 畳込み層\n",
        "        # 転置畳込み層の出力には checker board artifact というノイズが乗りやすいので，最後に畳込み層を通しておく\n",
        "        self.conv = nn.Conv2d(in_channels=8, out_channels=C, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = F.leaky_relu(self.fc2(z))\n",
        "        h = F.leaky_relu(self.fc1(h))\n",
        "        h = torch.reshape(h, (len(h), 64, self.H//8, self.W//8)) # 一列に並んだユニットを 64*(H/8)*(W/8) の特徴マップに並べ直す\n",
        "        h = F.leaky_relu(self.bn4(self.deconv4(h)))\n",
        "        h = F.leaky_relu(self.bn3(self.deconv3(h)))\n",
        "        h = F.leaky_relu(self.bn2(self.deconv2(h)))\n",
        "        h = F.leaky_relu(self.bn1(self.deconv1(h)))\n",
        "        y = torch.sigmoid(self.conv(h))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZpk13VjAg4U"
      },
      "source": [
        "##### 訓練データセットの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQx6BgaaAg4U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from mylib.data_io import TensorDataset\n",
        "\n",
        "\n",
        "# 再開モードの場合は，前回使用したデータセットをロードして使用する\n",
        "if RESTART_MODE:\n",
        "\n",
        "    # テンソルファイルを読み込み，前回使用したデータセットを用意\n",
        "    train_dataset = TensorDataset(filenames=[\n",
        "        os.path.join('./temp/', TRAIN_IMAGES_FILE)\n",
        "    ])\n",
        "    valid_dataset = TensorDataset(filenames=[\n",
        "        os.path.join('./temp/', VALID_IMAGES_FILE)\n",
        "    ])\n",
        "    train_size = len(train_dataset)\n",
        "    valid_size = len(valid_dataset)\n",
        "\n",
        "# そうでない場合は，新たにデータセットを読み込む\n",
        "else:\n",
        "\n",
        "    # テンソルファイルを読み込み, 訓練データセットを用意\n",
        "    dataset = TensorDataset(filenames=[\n",
        "        os.path.join(DATA_DIR, TRAIN_IMAGES_FILE)\n",
        "    ])\n",
        "\n",
        "    # 訓練データセットを分割し，一方を検証用に回す\n",
        "    dataset_size = len(dataset)\n",
        "    valid_size = int(0.002 * dataset_size) # 全体の 0.2% を検証用に -> tinyCelebA の画像は全部で 16000 枚なので，検証用画像は 16000*0.002=32 枚\n",
        "    train_size = dataset_size - valid_size # 残りの 99.8% を学習用に\n",
        "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "    # データセット情報をファイルに保存\n",
        "    torch.save(torch.cat([torch.unsqueeze(train_dataset[i], dim=0) for i in range(len(train_dataset))], dim=0), os.path.join('./temp/', TRAIN_IMAGES_FILE))\n",
        "    torch.save(torch.cat([torch.unsqueeze(valid_dataset[i], dim=0) for i in range(len(valid_dataset))], dim=0), os.path.join('./temp/', VALID_IMAGES_FILE))\n",
        "\n",
        "\n",
        "# 訓練データおよび検証用データをミニバッチに分けて使用するための「データローダ」を用意\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_edP9VwAg4V"
      },
      "source": [
        "##### 学習処理の実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5CUawp6Ag4V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from mylib.visualizers import LossVisualizer\n",
        "from mylib.data_io import show_images\n",
        "from mylib.utility import save_checkpoint, load_checkpoint\n",
        "\n",
        "\n",
        "# エポック番号\n",
        "INIT_EPOCH = 0 # 初期値\n",
        "LAST_EPOCH = INIT_EPOCH + N_EPOCHS # 最終値\n",
        "\n",
        "# ニューラルネットワークの作成\n",
        "enc_model = FaceEncoder(C=C, H=H, W=W, N=N).to(DEVICE)\n",
        "dec_model = FaceDecoder(C=C, H=H, W=W, N=N).to(DEVICE)\n",
        "\n",
        "# 最適化アルゴリズムの指定（ここでは SGD でなく Adam を使用）\n",
        "enc_optimizer = optim.Adam(enc_model.parameters(), eps=ADAM_EPS)\n",
        "dec_optimizer = optim.Adam(dec_model.parameters(), eps=ADAM_EPS)\n",
        "\n",
        "# 再開モードの場合は，前回チェックポイントから情報をロードして学習再開\n",
        "if RESTART_MODE:\n",
        "    INIT_EPOCH, LAST_EPOCH, enc_model, enc_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_ENC_MODEL, CHECKPOINT_ENC_OPT, N_EPOCHS, enc_model, enc_optimizer)\n",
        "    _, _, dec_model, dec_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DEC_MODEL, CHECKPOINT_DEC_OPT, N_EPOCHS, dec_model, dec_optimizer)\n",
        "    print('')\n",
        "\n",
        "# 損失関数\n",
        "loss_func = nn.MSELoss() # 平均二乗誤差損失を使用（これが最適とは限らない．平均絶対誤差損失 nn.L1Loss() なども考えられる）\n",
        "\n",
        "# 損失関数値を記録する準備\n",
        "loss_viz = LossVisualizer(['train loss', 'valid loss'], init_epoch=INIT_EPOCH)\n",
        "\n",
        "# 勾配降下法による繰り返し学習\n",
        "for epoch in range(INIT_EPOCH, LAST_EPOCH):\n",
        "\n",
        "    print('Epoch {0}:'.format(epoch + 1))\n",
        "\n",
        "    # 学習\n",
        "    enc_model.train()\n",
        "    dec_model.train()\n",
        "    sum_loss = 0\n",
        "    for X in tqdm(train_dataloader):\n",
        "        for param in enc_model.parameters():\n",
        "            param.grad = None\n",
        "        for param in dec_model.parameters():\n",
        "            param.grad = None\n",
        "        X = X.to(DEVICE)\n",
        "        with torch.amp.autocast_mode.autocast(enabled=USE_AMP, device_type='cuda', dtype=FLOAT_DTYPE):\n",
        "            Z = enc_model(X) # 入力画像 X を現在のエンコーダに入力し，特徴ベクトル Z を得る\n",
        "            Y = dec_model(Z) # 特徴ベクトル Z を現在のデコーダに入力し，復元画像 Y を得る\n",
        "            loss = loss_func(X, Y) # 損失関数の現在値を計算\n",
        "            LOSS_SCALER.scale(loss).backward() # 誤差逆伝播法により，個々のパラメータに関する損失関数の勾配（偏微分）を計算\n",
        "            LOSS_SCALER.step(enc_optimizer)\n",
        "            LOSS_SCALER.update() # 勾配に沿ってパラメータの値を更新\n",
        "            LOSS_SCALER.step(dec_optimizer)\n",
        "            LOSS_SCALER.update() # 同上\n",
        "            sum_loss += float(loss) * len(X)\n",
        "    avg_loss = sum_loss / train_size\n",
        "    loss_viz.add_value('train loss', avg_loss) # 訓練データに対する損失関数の値を記録\n",
        "    print('train loss = {0:.6f}'.format(avg_loss))\n",
        "\n",
        "    # 検証\n",
        "    enc_model.eval()\n",
        "    dec_model.eval()\n",
        "    sum_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for X in tqdm(valid_dataloader):\n",
        "            X = X.to(DEVICE)\n",
        "            Z = enc_model(X)\n",
        "            Y = dec_model(Z)\n",
        "            loss = loss_func(X, Y)\n",
        "            sum_loss += float(loss) * len(X)\n",
        "    avg_loss = sum_loss / valid_size\n",
        "    loss_viz.add_value('valid loss', avg_loss) # 検証用データに対する損失関数の値を記録\n",
        "    print('valid loss = {0:.6f}'.format(avg_loss))\n",
        "    print('')\n",
        "\n",
        "    # 学習経過の表示\n",
        "    if epoch == 0:\n",
        "        show_images(X.to('cpu').detach(), num=BATCH_SIZE, num_per_row=8, title='original', save_fig=False, save_dir=MODEL_DIR)\n",
        "    show_images(Y.to('cpu').detach(), num=BATCH_SIZE, num_per_row=8, title='epoch {0}'.format(epoch + 1), save_fig=False, save_dir=MODEL_DIR)\n",
        "\n",
        "    # 現在の学習状態を一時ファイル（チェックポイント）に保存\n",
        "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_ENC_MODEL, CHECKPOINT_ENC_OPT, epoch+1, enc_model, enc_optimizer)\n",
        "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DEC_MODEL, CHECKPOINT_DEC_OPT, epoch+1, dec_model, dec_optimizer)\n",
        "\n",
        "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
        "enc_model = enc_model.to('cpu')\n",
        "dec_model = dec_model.to('cpu')\n",
        "torch.save(enc_model.state_dict(), MODEL_FILE_ENC)\n",
        "torch.save(dec_model.state_dict(), MODEL_FILE_DEC)\n",
        "\n",
        "# 損失関数の記録をファイルに保存\n",
        "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'loss_history.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l6MbGR42mq-"
      },
      "source": [
        "##### 必要なら，現在の学習経過情報を自分のgoogle driveに退避\n",
        "- Google Colab以外の環境で使用することは想定していません．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8N1Hmng2nQN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# 退避先フォルダの名称\n",
        "# 自分のgoogle driveにおけるルートフォルダの直下にこの名前のフォルダが作成される\n",
        "# 既に存在する場合は一度削除した上で再作成される（削除したフォルダはgoogle driveのゴミ箱に移動するようです．必要に応じて完全に削除して下さい）\n",
        "DST_DIR_NAME = 'AI_advanced_temp'\n",
        "\n",
        "# tempフォルダをまるごとgoogle driveに退避\n",
        "MOUNT_POINT = '/content/drive'\n",
        "DST_DIR_PATH = os.path.join(MOUNT_POINT, 'MyDrive', DST_DIR_NAME)\n",
        "if not os.path.isdir(MOUNT_POINT):\n",
        "    drive.mount(MOUNT_POINT)\n",
        "if os.path.exists(DST_DIR_PATH):\n",
        "    !rm -r $DST_DIR_PATH\n",
        "!cp -r temp $DST_DIR_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JNO9sOzAg4V"
      },
      "source": [
        "##### 学習済みニューラルネットワークモデルのロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxTeA9bYAg4V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
        "dec_model = FaceDecoder(C=C, H=H, W=W, N=N)\n",
        "dec_model.load_state_dict(torch.load(MODEL_FILE_DEC, weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTna-4OrAg4W"
      },
      "source": [
        "##### テスト処理\n",
        "- 正規分布に従ってランダムサンプリングした乱数をデコーダに通して画像を生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s2WmadwAg4W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from mylib.data_io import show_images\n",
        "\n",
        "\n",
        "dec_model = dec_model.to(DEVICE)\n",
        "dec_model.eval()\n",
        "\n",
        "# 生成する画像の枚数\n",
        "n_gen = 32\n",
        "\n",
        "# 正規分布 N(Z_mu, Z_sigma^2) に従って適当に乱数ベクトルを作成（ Z_mu == 0, Z_sigma == 1 なら標準正規分布 ）\n",
        "Z_mu = 0 # 正規分布の平均\n",
        "Z_sigma = 50 # 正規分布の標準偏差\n",
        "Z = Z_sigma * torch.randn((n_gen, N)).to(DEVICE) + Z_mu\n",
        "\n",
        "# 乱数ベクトルをデコーダに入力し，その結果を表示\n",
        "with torch.inference_mode():\n",
        "    Y = dec_model(Z)\n",
        "    show_images(Y.to('cpu').detach(), num=n_gen, num_per_row=8, title='AE_sample_Face_generated', save_fig=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
